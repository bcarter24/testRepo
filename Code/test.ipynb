{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Install packages\n",
        "\n",
        "! pip install transformers datasets\n",
        "! pip install torch\n",
        "! pip install tensorflow\n",
        "! pip install tika\n",
        "! pip install nltk\n",
        "\n",
        "from transformers import pipeline\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "j5mTjVSyGBrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone GitHub repo with all of the data"
      ],
      "metadata": {
        "id": "AI_TmykUL0m1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bcarter24/testRepo.git"
      ],
      "metadata": {
        "id": "Aq8LbmzYI9nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Move to data directory"
      ],
      "metadata": {
        "id": "JdT4L-zbL6fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/testRepo/data/US\\ Federal\\ Code\\ of\\ Law/Cleaner_ungrouped/"
      ],
      "metadata": {
        "id": "u7-3Wd_BIMk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory1 = \"./Title18_Chapter44/\"\n",
        "directory2 = \"./Title26_Chapter53/\"\n",
        "\n",
        "document_array = []\n",
        "biggestSentArr = 0\n",
        "secondLargest = 0\n",
        "thirdLargest = 0\n",
        "\n",
        "#enumerate files in the directory into list for directory 1\n",
        "files_in_direc1 = os.listdir(directory1)\n",
        "#print(files_in_direc1)\n",
        "for file in files_in_direc1:\n",
        "  #check if file is valid\n",
        "  if os.path.isfile(os.path.join(directory1, file)):\n",
        "    #open file to read\n",
        "    file_handle = open(os.path.join(directory1, file), 'r+')\n",
        "    this_doc = str( file_handle.read())\n",
        "    file_handle.close()\n",
        "    #get rid of section part at the beginning but not section references later on\n",
        "    #this_doc = re.sub(r\"ยง\\s478\\.\", \"\", this_doc)\n",
        "    #this_doc = re.sub(r\"ยง\", \"\", this_doc)\n",
        "    \n",
        "    #tokenize\n",
        "    sentence_array = word_tokenize(this_doc,language='english', preserve_line=True)\n",
        "    document_array.append(sentence_array)\n",
        "    print(sentence_array)\n",
        "    print('length of sentence array is: ' + str(len(sentence_array)) + '\\n')\n",
        "    if len(sentence_array) > biggestSentArr :\n",
        "      thirdLargest = secondLargest\n",
        "      secondLargest = biggestSentArr\n",
        "      biggestSentArr = len(sentence_array)\n",
        "    #print('current length of Doc array is: ' + str(len(document_array)))\n",
        "\n",
        "print('\\nThere are ' + str(len(document_array)) + ' documents in the document_array')\n",
        "print('The largest Sentence Array so far has a token size of ' + str(biggestSentArr) + ' tokens.')\n",
        "print('The second largest Sentence Array so far has a token size of ' + str(secondLargest) + ' tokens.')\n",
        "print('The third largest Sentence Array so far has a token size of ' + str(thirdLargest) + ' tokens.\\n\\n\\n')\n",
        "\n",
        "\n",
        "#repeat for directory 2\n",
        "files_in_direc1 = os.listdir(directory2)\n",
        "#print(files_in_direc1)\n",
        "for file in files_in_direc1:\n",
        "  #check if file is valid\n",
        "  if os.path.isfile(os.path.join(directory2, file)):\n",
        "    #open file to read\n",
        "    file_handle = open(os.path.join(directory2, file), 'r+')\n",
        "    this_doc = str( file_handle.read())\n",
        "    file_handle.close()\n",
        "    #get rid of section part at the beginning but not section references later on\n",
        "    #this_doc = re.sub(r\"ยง\\s478\\.\", \"\", this_doc)\n",
        "    #this_doc = re.sub(r\"ยง\", \"\", this_doc)\n",
        "    \n",
        "    #tokenize\n",
        "    sentence_array = word_tokenize(this_doc,language='english', preserve_line=True)\n",
        "    document_array.append(sentence_array)\n",
        "    print(sentence_array)\n",
        "    print('length of sentence array is: ' + str(len(sentence_array)) + '\\n')\n",
        "    if len(sentence_array) > biggestSentArr :\n",
        "      thirdLargest = secondLargest\n",
        "      secondLargest = biggestSentArr\n",
        "      biggestSentArr = len(sentence_array)\n",
        "    #print('current length of Doc array is: ' + str(len(document_array)))\n",
        "\n",
        "print('\\nThere are ' + str(len(document_array)) + ' documents in the document_array')\n",
        "print('The largest Sentence Array so far has a token size of ' + str(biggestSentArr) + ' tokens.')\n",
        "print('The second largest Sentence Array so far has a token size of ' + str(secondLargest) + ' tokens.')\n",
        "print('The third largest Sentence Array so far has a token size of ' + str(thirdLargest) + ' tokens.\\n\\n\\n')"
      ],
      "metadata": {
        "id": "RHKiWS2NJ3al"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}